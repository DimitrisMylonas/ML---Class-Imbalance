# -*- coding: utf-8 -*-
"""PA_Weighting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1DrPjnUFJWf7iRvbhnjEKDuLZORJ0_8
"""

import pandas as pd
import numpy as np
import csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.calibration import CalibratedClassifierCV
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

data = np.loadtxt('heart.dat', unpack = True)
a = data.transpose()
header_list = ["1","2","3","4","5","6","7","8","9","10","11","12","13","target"]
np.savetxt('heart.csv', a, delimiter=',')

dataset = pd.read_csv("heart.csv", names=header_list)
dataset.target.replace({1:0, 2:1}, inplace = True)
dataset

frame = pd.DataFrame(dataset.target, columns=['target'])
print(frame['target'].value_counts())

X = dataset.iloc[:,0:13]
y = dataset.iloc[:,13]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=dataset.target)

fp = np.full((y_test.shape[0],1), 1)
fn = np.full((y_test.shape[0],1), 5)
tp = np.zeros((y_test.shape[0],1))
tn = np.zeros((y_test.shape[0],1))
cost_matrix = np.hstack((fp, fn, tp, tn))

cost_m = [[0 , 5], [1, 0]]
print("\nCost matrix: ", cost_m)

#Random Forest

print("Random Forest without weights")
RF_clf = RandomForestClassifier(n_estimators=10, random_state=0)
model = RF_clf.fit(X_train, y_train)
pred_test = model.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("\nRandom Forest with weights")
weights = np.zeros(y_train.shape[0])
weights[np.where(y_train == 1)] = 5;
weights[np.where(y_train == 0)] = 1;

model = RF_clf.fit(X_train, y_train, weights)
pred_test = RF_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

"""Παρατηρούμε ότι μετά την εφαρμογή του weighting, ο Random Forest μειώνει τον αριθμό των λανθασμένων προβλέψεων. Αυτό έχει ως αποτέλεσμα μείωση του loss από 77 σε 63."""

#SVM

print("SVM without weights")
SVM_clf = SVC(kernel='linear', probability=False, C=1)
model = SVM_clf.fit(X_train, y_train)
pred_test = model.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("\nSVM with weights")
SVM_clf = SVC(kernel='linear', probability=False, C=1, class_weight={0: 1, 1: 5})
model = SVM_clf.fit(X_train, y_train)
pred_test = model.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

"""Παρατηρούμε ότι μετά την εφαρμογή του weighting, ο SVM μειώνει τον αριθμό των λανθασμένων προβλέψεων στην κλάση με το μεγαλύτερο κόστος (από 10 σε 5), ενώ αυξάνονται οι προβλέψεις της κλάσης με το ελάχιστο κόστος (από 7 σε 15). Αυτό έχει ως αποτέλεσμα μείωση του loss από 57 σε 46."""

#Naive Bayes

print("Naive Bayes without weights")
NB_clf = GaussianNB()
model = NB_clf.fit(X_train, y_train)
pred_test = model.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("\nNaive Bayes with weights")
weights = np.zeros(y_train.shape[0])
weights[np.where(y_train == 1)] = 5;
weights[np.where(y_train == 0)] = 1;

model = NB_clf.fit(X_train, y_train, weights)
pred_test = NB_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, pred_test, target_names=t_names))
conf_m = confusion_matrix(y_test, pred_test).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)



"""Παρατηρούμε ότι μετά την εφαρμογή του weighting, ο Naive Bayes μειώνει κατά 1 τον αριθμό των λανθασμένων προβλέψεων στην κλάση με το μεγαλύτερο κόστος, ενώ αυξάνονται ελαφρώς οι προβλέψεις της κλάσης με το ελάχιστο κόστος (από 5 σε 9). Αυτό έχει ως αποτέλεσμα σημαντική το loss να μείνει σχεδόν αμετάβλητο (από 35 σε 34).

**Συνολικά, παρατηρούμε ότι για το συγκεκριμένο σύνολο δεδομένων ο ταξινομητής Naive Bayes πετυχαίνει τα καλύτερα αποτελέσματα (μικρότερο loss) με ή χωρίς εφαρμοφή weighting (35 και 34 αντίστοιχα), ενώ είναι αυτός που επηρεάζετε λιγότερο από την εφαρμογή της τεχνικής αυτής. O Random Forest βελτιώνει το loss του αλλά παραμένει ο χειρότερος σε απόδοση εκ των τριών (77 και 63 αντίστοιχα). Τέλος, ο SVM φαίνεται να πετυχαίνει την μεγαλύτερη μείωση του loss έπειτα απο την εφαρμογή του weighting συγκριτικά με τους άλλους δυο Classifiers (από 57 σε 40). **
"""
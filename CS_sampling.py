# -*- coding: utf-8 -*-
"""PA_Sampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DmCEvMGnMbe_yIZtZtH6KdN2V6jwfgGE
"""

#rebalancing
!pip install imbalanced-learn

import pandas as pd
import numpy as np
import csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.calibration import CalibratedClassifierCV
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

data = np.loadtxt('heart.dat', unpack = True)
a = data.transpose()
header_list = ["1","2","3","4","5","6","7","8","9","10","11","12","13","target"]
np.savetxt('heart.csv', a, delimiter=',')

dataset = pd.read_csv("heart.csv", names=header_list)
dataset.target.replace({1:0, 2:1}, inplace = True)

dataset

frame = pd.DataFrame(dataset.target, columns=['target'])
print(frame['target'].value_counts())

X = dataset.iloc[:,0:13]
y = dataset.iloc[:,13]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=dataset.target)

fp = np.full((y_test.shape[0],1), 1)
fn = np.full((y_test.shape[0],1), 5)
tp = np.zeros((y_test.shape[0],1))
tn = np.zeros((y_test.shape[0],1))
cost_matrix = np.hstack((fp, fn, tp, tn))

cost_m = [[0 , 5], [1, 0]]
print("\nCost matrix: ", cost_m)

#RANDOM FOREST

RF_clf = RandomForestClassifier(n_estimators=100, random_state=0)

print("Random forest without sampling")
print(Counter(y_train))

model = RF_clf.fit(X_train, y_train)
y_pred = RF_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("Random Forest with oversampling")
sampler = RandomOverSampler(sampling_strategy={0: 105, 1: 420}, random_state=1)
X_rs, y_rs = sampler.fit_resample(X_train, y_train)
print(Counter(y_rs))

model = RF_clf.fit(X_rs, y_rs)
y_pred = RF_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

"""Παρατηρούμε ότι μετά την εφαρμογή του oversampling, ο Random Forest μειώνει ελαφρώς τον αριθμό των λανθασμένων προβλέψεων στην κλάση με το μεγαλύτερο κόστος (από 10 σε 8), ενώ αυξάνονται ελαφρώς οι προβλέψεις της κλάσης με το ελάχιστο κόστος (από 8 σε 5). Αυτό έχει ως αποτέλεσμα τη μείωση του loss από 58 σε 50."""

#SVM

SVM_clf = SVC(kernel='linear', C=1)

print("SVM without sampling")
print(Counter(y_train))

model = SVM_clf.fit(X_train, y_train)
y_pred = SVM_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("SVM with oversampling")
sampler = RandomOverSampler(sampling_strategy={0: 105, 1: 420}, random_state=1)
X_rs, y_rs = sampler.fit_resample(X_train, y_train)
print(Counter(y_rs))

model = SVM_clf.fit(X_rs, y_rs)
y_pred = SVM_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

"""Παρατηρούμε ότι μετά την εφαρμογή του oversampling, ο SVM μειώνει στο μισό τον αριθμό των λανθασμένων προβλέψεων στην κλάση με το μεγαλύτερο κόστος (από 10 σε 5), ενώ διπλασιάζονται οι προβλέψεις της κλάσης με το ελάχιστο κόστος (από 7 σε 14). Αυτό έχει ως αποτέλεσμα σημαντική μείωση του loss από 57 σε 39."""

#Naive Bayes

NB_clf = GaussianNB()

print("Naive Bayes without sampling")
print(Counter(y_train))

model = NB_clf.fit(X_train, y_train)
y_pred = NB_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

print("Naive Bayes with oversampling")
sampler = RandomOverSampler(sampling_strategy={0: 105, 1: 420}, random_state=1)
X_rs, y_rs = sampler.fit_resample(X_train, y_train)
print(Counter(y_rs))

model = NB_clf.fit(X_rs, y_rs)
y_pred = NB_clf.predict(X_test)
t_names = ['Absence', 'Presence']
print(classification_report(y_test, y_pred, target_names=t_names))
conf_m = confusion_matrix(y_test, y_pred).T 
print(conf_m)
loss = np.sum(conf_m * cost_m)
print("%d\n" %loss)

"""Παρατηρούμε ότι μετά την εφαρμογή του oversampling, ο Naive Bayes μειώνει κατά 1 τον αριθμό των λανθασμένων προβλέψεων στην κλάση με το μεγαλύτερο κόστος (από 6 σε 5), ενώ αυξάνονται οι προβλέψεις της κλάσης με το ελάχιστο κόστος (από 5 σε 9). Αυτό έχει ως αποτέλεσμα σημαντική το loss να μείνει σχεδόν αμετάβλητο (από 35 σε 34).

**Συνολικά, παρατηρούμε ότι για το συγκεκριμένο σύνολο δεδομένων ο ταξινομητής Naive Bayes πετυχαίνει τα καλύτερα αποτελέσματα (μικρότερο loss) με ή χωρίς εφαρμοφή weighting (35 και 34 αντίστοιχα), ενώ είναι αυτός που επηρεάζετε λιγότερο από την εφαρμογή της τεχνικής αυτής. O Random Forest βελτιώνει το loss του αλλά παραμένει ο χειρότερος σε απόδοση εκ των τριών (58 και 50 αντίστοιχα). Τέλος, ο SVM φαίνεται να πετυχαίνει την μεγαλύτερη μείωση του loss έπειτα απο την εφαρμογή του weighting συγκριτικά με τους άλλους δυο Classifiers (από 57 σε 39). **
"""